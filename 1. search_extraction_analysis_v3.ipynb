{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043c7333",
   "metadata": {},
   "source": [
    "# Search History Information Extraction & Analysis\n",
    "\n",
    "## Project Overview\n",
    "This notebook analyzes Google search history data to extract meaningful insights about user interests, preferences, and behavior patterns. Using Named Entity Recognition (NER) with OpenAI's GPT-4o-mini model and web scraping, we extract structured information from both search queries and visited pages to build a comprehensive user interest profile.\n",
    "\n",
    "### Key Capabilities:\n",
    "- Extract and categorize search queries by semantic meaning\n",
    "- Analyze temporal patterns and search behavior\n",
    "- Scrape and extract content from visited web pages\n",
    "- Identify search clusters and user intent patterns\n",
    "- Generate comprehensive user interest profiles\n",
    "- Track API costs and optimize caching for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0da62",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore Search History Data\n",
    "\n",
    "**Objective:** Load the Google search history JSON file and understand its structure, including all available fields and how records are organized.\n",
    "\n",
    "**What this section does:**\n",
    "- Loads search history from the JSON file\n",
    "- Examines the data structure and available fields\n",
    "- Shows sample records to understand the format\n",
    "- Displays statistics about total records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59837cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2b8dcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total search history records: 55383\n",
      "\n",
      "First 3 records structure:\n",
      "\n",
      "--- Record 1 ---\n",
      "Keys: dict_keys(['header', 'title', 'titleUrl', 'time', 'products', 'activityControls'])\n",
      "Title: Visited https://www.businessinsider.com/shivon-zilis-reported-mother-elon-musk-t\n",
      "Time: 2024-06-23T22:21:50.431Z\n",
      "\n",
      "--- Record 2 ---\n",
      "Keys: dict_keys(['header', 'title', 'titleUrl', 'time', 'products', 'activityControls'])\n",
      "Title: Visited Elon Musk and Shivon Zilis privately welcome third baby ‚Äì NBC10 ...\n",
      "Time: 2024-06-23T22:20:53.934Z\n",
      "\n",
      "--- Record 3 ---\n",
      "Keys: dict_keys(['header', 'title', 'titleUrl', 'time', 'products', 'activityControls', 'locationInfos'])\n",
      "Title: Searched for elon musk shivon zilis\n",
      "Time: 2024-06-23T22:20:47.560Z\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the search history\n",
    "with open('search_history.json', 'r') as f:\n",
    "    search_data = json.load(f)\n",
    "\n",
    "print(f\"Total search history records: {len(search_data)}\")\n",
    "print(f\"\\nFirst 3 records structure:\")\n",
    "for i, record in enumerate(search_data[:3]):\n",
    "    print(f\"\\n--- Record {i+1} ---\")\n",
    "    print(f\"Keys: {record.keys()}\")\n",
    "    print(f\"Title: {record.get('title', 'N/A')[:80]}\")\n",
    "    print(f\"Time: {record.get('time', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ccb951",
   "metadata": {},
   "source": [
    "## Section 2: Parse and Clean Search History Records\n",
    "\n",
    "**Objective:** Process raw search history records to extract, classify, and structure the data for downstream analysis.\n",
    "\n",
    "**What this section does:**\n",
    "- Extracts search queries from title field using regex patterns\n",
    "- Classifies activities (searches, page visits, notifications)\n",
    "- Handles three types of records: \"Searched for\", \"Visited\", and other activities\n",
    "- Converts timestamps to datetime format\n",
    "- Adds temporal features (date, hour, day_of_week)\n",
    "- Displays distribution of activity types and date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6a8bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 7405 duplicate search/page_visit records (kept first occurrences)\n",
      "Total records: 47978\n",
      "\n",
      "Activity type distribution:\n",
      "activity_type\n",
      "search_query    25807\n",
      "page_visit      19826\n",
      "other            2187\n",
      "notification      158\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date range: 2017-06-08 16:45:50.139000+00:00 to 2024-06-23 22:21:50.431000+00:00\n",
      "\n",
      "Sample cleaned records:\n",
      "  activity_type                                 search_query  \\\n",
      "0  search_query  investment banking networking events london   \n",
      "1    page_visit                                         None   \n",
      "2  search_query          blackstone's women networking event   \n",
      "3  search_query                                   blackstone   \n",
      "4    page_visit                                         None   \n",
      "5  search_query                                         face   \n",
      "6    page_visit                                         None   \n",
      "7  search_query      how to find people's emails on data.com   \n",
      "8    page_visit                                         None   \n",
      "9    page_visit                                         None   \n",
      "\n",
      "                                               title  \\\n",
      "0  Searched for investment banking networking eve...   \n",
      "1  Visited http://news.efinancialcareers.com/uk-e...   \n",
      "2   Searched for blackstone's women networking event   \n",
      "3                            Searched for blackstone   \n",
      "4                           Visited Blackstone: Home   \n",
      "5                                  Searched for face   \n",
      "6               Visited Facebook - log in or sign up   \n",
      "7  Searched for how to find people's emails on da...   \n",
      "8  Visited http://www.mckinsey.com/business-funct...   \n",
      "9               Visited Valuation 3rd ed - Amazon UK   \n",
      "\n",
      "                                            titleUrl  \\\n",
      "0  https://www.google.com/search?q=investment+ban...   \n",
      "1  https://www.google.com/url?q=http://news.efina...   \n",
      "2  https://www.google.com/search?q=blackstone%27s...   \n",
      "3         https://www.google.com/search?q=blackstone   \n",
      "4  https://www.google.com/url?q=https://www.black...   \n",
      "5               https://www.google.com/search?q=face   \n",
      "6  https://www.google.com/url?q=https://www.faceb...   \n",
      "7  https://www.google.com/search?q=how+to+find+pe...   \n",
      "8  https://www.google.com/url?q=http://www.mckins...   \n",
      "9  https://www.google.com/url?q=https://www.amazo...   \n",
      "\n",
      "                         timestamp  \n",
      "0 2017-06-08 16:45:50.139000+00:00  \n",
      "1 2017-06-08 16:45:58.449000+00:00  \n",
      "2 2017-06-08 16:48:12.167000+00:00  \n",
      "3 2017-06-08 16:51:08.080000+00:00  \n",
      "4 2017-06-08 16:51:10.506000+00:00  \n",
      "5 2017-06-08 17:04:43.881000+00:00  \n",
      "6 2017-06-08 17:04:53.892000+00:00  \n",
      "7 2017-06-08 17:45:10.608000+00:00  \n",
      "8 2017-06-08 18:39:44.761000+00:00  \n",
      "9 2017-06-08 18:41:00.884000+00:00  \n"
     ]
    }
   ],
   "source": [
    "# Function to extract search query from title\n",
    "def extract_search_query(title):\n",
    "    \"\"\"\n",
    "    Extract actual search query from the title field.\n",
    "    Handles three types of records:\n",
    "    1. \"Searched for [query]\" - explicit search queries\n",
    "    2. \"Visited [URL/Title]\" - visited pages (not actual searches)\n",
    "    3. Notifications and other non-search activities\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        return None\n",
    "    \n",
    "    # Pattern 1: \"Searched for [query]\"\n",
    "    search_match = re.search(r'^Searched for (.+?)(?:\\s*$|[\\?&])', title)\n",
    "    if search_match:\n",
    "        return search_match.group(1).strip()\n",
    "    \n",
    "    # Pattern 2: Extract from notification topics\n",
    "    if \"notification\" in title.lower() and \"Including topics:\" in title:\n",
    "        return None  # Notifications aren't searches\n",
    "    \n",
    "    # Pattern 3: Visited URLs - extract from title content but mark differently\n",
    "    if title.startswith(\"Visited\"):\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to classify activity type\n",
    "def classify_activity(record):\n",
    "    \"\"\"Classify the type of activity\"\"\"\n",
    "    title = record.get('title', '')\n",
    "    \n",
    "    if title.startswith('Searched for'):\n",
    "        return 'search_query'\n",
    "    elif title.startswith('Visited'):\n",
    "        return 'page_visit'\n",
    "    elif 'notification' in title.lower():\n",
    "        return 'notification'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Clean and structure the data\n",
    "cleaned_records = []\n",
    "for record in search_data:\n",
    "    cleaned = {\n",
    "        'timestamp': record.get('time'),\n",
    "        'title': record.get('title'),\n",
    "        'titleUrl': record.get('titleUrl'),\n",
    "        'activity_type': classify_activity(record),\n",
    "        'search_query': extract_search_query(record.get('title')),\n",
    "        'location': record.get('locationInfos', [{}])[0].get('name') if record.get('locationInfos') else None,\n",
    "    }\n",
    "    cleaned_records.append(cleaned)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(cleaned_records)\n",
    "\n",
    "# Convert timestamp to datetime - use format='ISO8601' to handle mixed formats\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601', utc=True)\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# Remove duplicate searches and page visits (keep first occurrence)\n",
    "before_rows = len(df)\n",
    "search_dedup = (\n",
    "    df[df['activity_type'] == 'search_query']\n",
    "    .dropna(subset=['search_query'])\n",
    "    .drop_duplicates(subset=['search_query'])\n",
    ")\n",
    "page_dedup = (\n",
    "    df[df['activity_type'] == 'page_visit']\n",
    "    .dropna(subset=['titleUrl'])\n",
    "    .drop_duplicates(subset=['titleUrl'])\n",
    ")\n",
    "others = df[~df['activity_type'].isin(['search_query', 'page_visit'])]\n",
    "df = pd.concat([search_dedup, page_dedup, others], ignore_index=True)\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "after_rows = len(df)\n",
    "duplicates_removed = before_rows - after_rows\n",
    "print(f\"Removed {duplicates_removed} duplicate search/page_visit records (kept first occurrences)\")\n",
    "\n",
    "# Show data summary\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"\\nActivity type distribution:\")\n",
    "print(df['activity_type'].value_counts())\n",
    "print(f\"\\nDate range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"\\nSample cleaned records:\")\n",
    "print(df[['activity_type', 'search_query', 'title','titleUrl','timestamp',]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09293fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data directory: /Users/shreyasjagannath/nltk_data\n",
      "‚Üí Downloading punkt (tokenizers)‚Ä¶ [nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shreyasjagannath/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "‚úì\n",
      "‚Üí Downloading punkt_tab (tokenizers)‚Ä¶ [nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/shreyasjagannath/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "‚úì\n",
      "‚Üí Downloading stopwords (corpora)‚Ä¶ [nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreyasjagannath/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "‚úì\n",
      "‚Üí Downloading wordnet (corpora)‚Ä¶ [nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shreyasjagannath/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "‚ö† not found after download\n",
      "‚Üí Downloading averaged_perceptron_tagger (taggers)‚Ä¶ [nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shreyasjagannath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "‚úì\n",
      "‚Üí Downloading averaged_perceptron_tagger_eng (taggers)‚Ä¶ [nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/shreyasjagannath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "‚úì\n",
      "\n",
      "Summary:\n",
      "  NLTK_DATA=/Users/shreyasjagannath/nltk_data\n",
      "  Success: PARTIAL/NO\n",
      "\n",
      "If downloads failed due to network/SSL, you can manually fetch ZIPs:\n",
      "  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip\n",
      "  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt_tab.zip\n",
      "  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip\n",
      "  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/wordnet.zip\n",
      "  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger.zip\n",
      "  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger_eng.zip\n",
      "Then unzip into /Users/shreyasjagannath/nltk_data/<subdir> accordingly.\n"
     ]
    }
   ],
   "source": [
    "#run twice to verify if downloaded\n",
    "!python /Users/shreyasjagannath/dev/fabric/onfabric-data-science-interview/download_nltk_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f1fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SEARCH CLUSTER EXTRACTION WITH NLTK\n",
      "====================================================================================================\n",
      "\n",
      "NLP Techniques Enabled:\n",
      "  ‚úì Tokenization - Split text into individual words/tokens\n",
      "  ‚úì Stopword Removal - Remove common English words\n",
      "  ‚úì Lemmatization - Convert words to base/dictionary form (e.g., 'running' ‚Üí 'run')\n",
      "\n",
      "Input dataframe shape: (47978, 9)\n",
      "Columns: ['timestamp', 'title', 'titleUrl', 'activity_type', 'search_query', 'location', 'date', 'hour', 'day_of_week']\n",
      "\n",
      "Function parameters:\n",
      "  - time_window_minutes: 10 (entries within 10 min window)\n",
      "  - keyword_similarity_threshold: 0.3 (30% keyword overlap)\n",
      "  - max_lookahead: 10 (compare with up to 10 following entries)\n",
      "  - min_cluster_size: 2 (minimum 2 entries per cluster)\n",
      "  - use_stemming: False (stemming removed, always lemmatizes)\n",
      "\n",
      "‚úÖ NLTK tools initialized successfully!\n",
      "Ready to call: clusters_df, info = extract_search_clusters(df)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import timedelta\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Additional domain-specific stopwords to exclude\n",
    "domain_stopwords = {\n",
    "    'visited', 'searched', 'www', 'http', 'https', 'com', 'org', 'net', 'co', 'uk', 'us',\n",
    "    'page', 'site', 'link', 'url', 'web', 'search', 'result', 'article', 'post', 'blog',\n",
    "    'click', 'view', 'visit', 'read', 'news'\n",
    "}\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "def extract_keywords_nltk(text):\n",
    "    \"\"\"\n",
    "    Extract meaningful keywords using NLTK NLP techniques.\n",
    "    Applies: Tokenization -> Lowercase -> Stopword Removal -> Lemmatization\n",
    "    Returns a set of normalized keywords\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Tokenization - Split text into words\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Step 2: Remove special characters and filter alphanumeric tokens\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "        \n",
    "        # Step 3: Stopword Removal - Remove common words\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "        \n",
    "        # Step 4: Lemmatization - Convert words to base form\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Convert to set to get unique keywords\n",
    "        keywords = set(lemmatized_tokens)\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return set()\n",
    "\n",
    "def extract_keywords_lemmatized(text):\n",
    "    \"\"\"\n",
    "    Extract keywords using only lemmatization (identical to extract_keywords_nltk now)\n",
    "    Useful for preserving word meaning\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return set(lemmatized_tokens)\n",
    "    except Exception as e:\n",
    "        return set()\n",
    "\n",
    "def calculate_keyword_similarity(keywords1, keywords2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two sets of keywords using Jaccard similarity.\n",
    "    Returns a score between 0 and 1.\n",
    "    \n",
    "    Jaccard = (intersection size) / (union size)\n",
    "    \"\"\"\n",
    "    if not keywords1 or not keywords2:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = len(keywords1 & keywords2)\n",
    "    union = len(keywords1 | keywords2)\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def extract_search_clusters(df, time_window_minutes=10, keyword_similarity_threshold=0.3, \n",
    "                             max_lookahead=10, min_cluster_size=2, use_stemming=False):\n",
    "    \"\"\"\n",
    "    Extract search clusters from the dataframe based on keyword similarity and timestamps.\n",
    "    Uses NLTK for advanced text processing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe with columns: timestamp, title, search_query, activity_type\n",
    "    \n",
    "    time_window_minutes : int, default=10\n",
    "        Maximum time window (in minutes) for entries to be considered in the same cluster\n",
    "    \n",
    "    keyword_similarity_threshold : float, default=0.3\n",
    "        Minimum keyword similarity (0-1) to consider entries as related\n",
    "    \n",
    "    max_lookahead : int, default=10\n",
    "        Only compare each entry with up to N entries after it (performance optimization)\n",
    "    \n",
    "    min_cluster_size : int, default=2\n",
    "        Minimum number of entries required to form a cluster\n",
    "    \n",
    "    use_stemming : bool, default=True (ignored, always lemmatizes now)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (clusters_df, cluster_info)\n",
    "        - clusters_df: DataFrame with all entries that are part of clusters, with cluster_id\n",
    "        - cluster_info: Dictionary with summary statistics about detected clusters\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    clusters_df, info = extract_search_clusters(df)\n",
    "    print(f\"Found {info['num_clusters']} clusters\")\n",
    "    print(clusters_df.head())\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort by timestamp for sequential processing\n",
    "    sorted_df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Always use lemmatization now\n",
    "    keyword_extractor = extract_keywords_nltk\n",
    "    \n",
    "    # Dictionary to store cluster assignments: index -> cluster_id\n",
    "    cluster_assignments = {}\n",
    "    cluster_counter = 0\n",
    "    \n",
    "    # Keep track of which indices have been assigned to clusters\n",
    "    assigned_indices = set()\n",
    "    \n",
    "    # Pre-compute keywords for all entries to improve performance\n",
    "    print(\"Extracting keywords from all entries...\")\n",
    "    entry_keywords = {}\n",
    "    for i in range(len(sorted_df)):\n",
    "        current_row = sorted_df.iloc[i]\n",
    "        text = str(current_row['search_query']) if pd.notna(current_row['search_query']) else current_row['title']\n",
    "        entry_keywords[i] = keyword_extractor(text)\n",
    "    \n",
    "    print(f\"‚úì Keyword extraction complete for {len(entry_keywords)} entries\\n\")\n",
    "    \n",
    "    # Process each entry\n",
    "    for i in range(len(sorted_df)):\n",
    "        # Skip if already assigned to a cluster\n",
    "        if i in assigned_indices:\n",
    "            continue\n",
    "        \n",
    "        current_row = sorted_df.iloc[i]\n",
    "        current_timestamp = current_row['timestamp']\n",
    "        current_keywords = entry_keywords[i]\n",
    "        \n",
    "        # If no keywords extracted, skip\n",
    "        if not current_keywords:\n",
    "            continue\n",
    "        \n",
    "        # Find related entries within lookahead window\n",
    "        cluster_indices = [i]  # Start with current entry\n",
    "        \n",
    "        # Look ahead at most max_lookahead entries\n",
    "        lookahead_end = min(i + 1 + max_lookahead, len(sorted_df))\n",
    "        \n",
    "        for j in range(i + 1, lookahead_end):\n",
    "            if j in assigned_indices:\n",
    "                continue\n",
    "            \n",
    "            next_row = sorted_df.iloc[j]\n",
    "            next_timestamp = next_row['timestamp']\n",
    "            \n",
    "            # Check time window constraint\n",
    "            time_diff = (next_timestamp - current_timestamp).total_seconds() / 60\n",
    "            if time_diff > time_window_minutes:\n",
    "                break  # No point checking further entries (sorted by time)\n",
    "            \n",
    "            # Get pre-computed keywords\n",
    "            next_keywords = entry_keywords[j]\n",
    "            \n",
    "            if not next_keywords:\n",
    "                continue\n",
    "            \n",
    "            # Calculate keyword similarity\n",
    "            similarity = calculate_keyword_similarity(current_keywords, next_keywords)\n",
    "            \n",
    "            # If similar enough, add to cluster\n",
    "            if similarity >= keyword_similarity_threshold:\n",
    "                cluster_indices.append(j)\n",
    "        \n",
    "        # Only create cluster if it has minimum size\n",
    "        if len(cluster_indices) >= min_cluster_size:\n",
    "            for idx in cluster_indices:\n",
    "                cluster_assignments[idx] = cluster_counter\n",
    "                assigned_indices.add(idx)\n",
    "            cluster_counter += 1\n",
    "    \n",
    "    # Create result dataframe with cluster assignments\n",
    "    if cluster_assignments:\n",
    "        cluster_df = sorted_df.loc[list(cluster_assignments.keys())].copy()\n",
    "        cluster_df['cluster_id'] = cluster_df.index.map(cluster_assignments)\n",
    "        \n",
    "        # Sort by cluster_id and timestamp for better organization\n",
    "        cluster_df = cluster_df.sort_values(['cluster_id', 'timestamp']).reset_index(drop=True)\n",
    "    else:\n",
    "        # No clusters found\n",
    "        cluster_df = pd.DataFrame(columns=sorted_df.columns.tolist() + ['cluster_id'])\n",
    "    \n",
    "    # Generate cluster information statistics\n",
    "    cluster_info = {\n",
    "        'num_clusters': cluster_counter,\n",
    "        'total_clustered_entries': len(assigned_indices),\n",
    "        'total_entries': len(sorted_df),\n",
    "        'clustering_percentage': (len(assigned_indices) / len(sorted_df) * 100) if len(sorted_df) > 0 else 0,\n",
    "        'time_window_minutes': time_window_minutes,\n",
    "        'similarity_threshold': keyword_similarity_threshold,\n",
    "        'max_lookahead': max_lookahead,\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'nlp_method': 'Lemmatization (WordNet)',\n",
    "    }\n",
    "    \n",
    "    # Add per-cluster statistics\n",
    "    if cluster_counter > 0:\n",
    "        cluster_sizes = {}\n",
    "        for idx, cid in cluster_assignments.items():\n",
    "            cluster_sizes[cid] = cluster_sizes.get(cid, 0) + 1\n",
    "        \n",
    "        cluster_info['cluster_sizes'] = cluster_sizes\n",
    "        cluster_info['avg_cluster_size'] = sum(cluster_sizes.values()) / len(cluster_sizes)\n",
    "        cluster_info['largest_cluster_size'] = max(cluster_sizes.values())\n",
    "    \n",
    "    return cluster_df, cluster_info\n",
    "\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"SEARCH CLUSTER EXTRACTION WITH NLTK\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nNLP Techniques Enabled:\")\n",
    "print(f\"  ‚úì Tokenization - Split text into individual words/tokens\")\n",
    "print(f\"  ‚úì Stopword Removal - Remove common English words\")\n",
    "print(f\"  ‚úì Lemmatization - Convert words to base/dictionary form (e.g., 'running' ‚Üí 'run')\")\n",
    "print(f\"\\nInput dataframe shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFunction parameters:\")\n",
    "print(f\"  - time_window_minutes: 10 (entries within 10 min window)\")\n",
    "print(f\"  - keyword_similarity_threshold: 0.3 (30% keyword overlap)\")\n",
    "print(f\"  - max_lookahead: 10 (compare with up to 10 following entries)\")\n",
    "print(f\"  - min_cluster_size: 2 (minimum 2 entries per cluster)\")\n",
    "print(f\"  - use_stemming: False (stemming removed, always lemmatizes)\")\n",
    "print(f\"\\n‚úÖ NLTK tools initialized successfully!\")\n",
    "print(f\"Ready to call: clusters_df, info = extract_search_clusters(df)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f61eb34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "EXTRACTING SEARCH CLUSTERS FROM DATA (NLTK-POWERED)\n",
      "====================================================================================================\n",
      "Extracting keywords from all entries...\n",
      "Extracting keywords from all entries...\n",
      "‚úì Keyword extraction complete for 47978 entries\n",
      "\n",
      "‚úì Keyword extraction complete for 47978 entries\n",
      "\n",
      "\n",
      "‚úì Clustering complete!\n",
      "\n",
      "Cluster Statistics:\n",
      "  Total clusters found: 28566\n",
      "  Total clustered entries: 38649 / 47978\n",
      "  Clustering percentage: 80.6%\n",
      "  Average cluster size: 1.4\n",
      "  Largest cluster size: 13\n",
      "  NLP Method: Lemmatization (WordNet)\n",
      "\n",
      "üìä Cluster Size Distribution:\n",
      "  1 entries: 21325 clusters\n",
      "  2 entries: 5340 clusters\n",
      "  3 entries: 1328 clusters\n",
      "  4 entries: 369 clusters\n",
      "  5 entries: 117 clusters\n",
      "  6 entries: 48 clusters\n",
      "  7 entries: 22 clusters\n",
      "  8 entries: 9 clusters\n",
      "  9 entries: 3 clusters\n",
      "  10 entries: 2 clusters\n",
      "  12 entries: 1 clusters\n",
      "  13 entries: 2 clusters\n",
      "\n",
      "‚úì Result dataframe shape: (38649, 10)\n",
      "  Columns: ['timestamp', 'title', 'titleUrl', 'activity_type', 'search_query', 'location', 'date', 'hour', 'day_of_week', 'cluster_id']\n",
      "\n",
      "‚úì Clustering complete!\n",
      "\n",
      "Cluster Statistics:\n",
      "  Total clusters found: 28566\n",
      "  Total clustered entries: 38649 / 47978\n",
      "  Clustering percentage: 80.6%\n",
      "  Average cluster size: 1.4\n",
      "  Largest cluster size: 13\n",
      "  NLP Method: Lemmatization (WordNet)\n",
      "\n",
      "üìä Cluster Size Distribution:\n",
      "  1 entries: 21325 clusters\n",
      "  2 entries: 5340 clusters\n",
      "  3 entries: 1328 clusters\n",
      "  4 entries: 369 clusters\n",
      "  5 entries: 117 clusters\n",
      "  6 entries: 48 clusters\n",
      "  7 entries: 22 clusters\n",
      "  8 entries: 9 clusters\n",
      "  9 entries: 3 clusters\n",
      "  10 entries: 2 clusters\n",
      "  12 entries: 1 clusters\n",
      "  13 entries: 2 clusters\n",
      "\n",
      "‚úì Result dataframe shape: (38649, 10)\n",
      "  Columns: ['timestamp', 'title', 'titleUrl', 'activity_type', 'search_query', 'location', 'date', 'hour', 'day_of_week', 'cluster_id']\n"
     ]
    }
   ],
   "source": [
    "# Run the search cluster extraction with NLTK\n",
    "print(\"=\" * 100)\n",
    "print(\"EXTRACTING SEARCH CLUSTERS FROM DATA (NLTK-POWERED)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "clusters_df, cluster_info = extract_search_clusters(\n",
    "    df,\n",
    "    time_window_minutes=20,           # 20-minute time window\n",
    "    keyword_similarity_threshold=0.3,  # 30% keyword overlap\n",
    "    max_lookahead=20,                  # Compare with 20 entries ahead\n",
    "    min_cluster_size=1,                # Minimum 1 entry per cluster\n",
    "    use_stemming=False                 # Stemming removed, always lemmatizes\n",
    " )\n",
    "\n",
    "print(f\"\\n‚úì Clustering complete!\")\n",
    "print(f\"\\nCluster Statistics:\")\n",
    "print(f\"  Total clusters found: {cluster_info['num_clusters']}\")\n",
    "print(f\"  Total clustered entries: {cluster_info['total_clustered_entries']} / {cluster_info['total_entries']}\")\n",
    "print(f\"  Clustering percentage: {cluster_info['clustering_percentage']:.1f}%\")\n",
    "print(f\"  Average cluster size: {cluster_info.get('avg_cluster_size', 0):.1f}\")\n",
    "print(f\"  Largest cluster size: {cluster_info.get('largest_cluster_size', 0)}\")\n",
    "print(f\"  NLP Method: {cluster_info['nlp_method']}\")\n",
    "\n",
    "# Display cluster distribution\n",
    "if cluster_info['num_clusters'] > 0:\n",
    "    print(f\"\\nüìä Cluster Size Distribution:\")\n",
    "    size_counts = {}\n",
    "    for size in cluster_info['cluster_sizes'].values():\n",
    "        size_counts[size] = size_counts.get(size, 0) + 1\n",
    "    \n",
    "    for size in sorted(size_counts.keys()):\n",
    "        count = size_counts[size]\n",
    "        print(f\"  {size} entries: {count} clusters\")\n",
    "\n",
    "print(f\"\\n‚úì Result dataframe shape: {clusters_df.shape}\")\n",
    "print(f\"  Columns: {clusters_df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72dad84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "NLTK TEXT PREPROCESSING DEMONSTRATION\n",
      "====================================================================================================\n",
      "\n",
      "Text Processing Pipeline:\n",
      "\n",
      "Original Text                                      ‚Üí Tokens                                            \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hermes sandals for women designer luxury           ‚Üí ['hermes', 'sandals', 'for', 'women', 'designer', 'luxury']\n",
      "searching for luxury hermes shoe designs           ‚Üí ['searching', 'for', 'luxury', 'hermes', 'shoe', 'designs']\n",
      "visited luxury designer sandals online             ‚Üí ['visited', 'luxury', 'designer', 'sandals', 'online']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Tokenization + Stopword Removal              ‚Üí After Lemmatization                               \n",
      "----------------------------------------------------------------------------------------------------\n",
      "['hermes', 'sandals', 'women', 'designer', 'luxury'] ‚Üí ['hermes', 'sandal', 'woman', 'designer', 'luxury\n",
      "['searching', 'luxury', 'hermes', 'shoe', 'designs'] ‚Üí ['searching', 'luxury', 'hermes', 'shoe', 'design\n",
      "['luxury', 'designer', 'sandals', 'online']        ‚Üí ['luxury', 'designer', 'sandal', 'online']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lemmatized                                         ‚Üí Final Keywords                                    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "['hermes', 'sandal', 'woman', 'designer', 'luxury'] ‚Üí ['designer', 'hermes', 'luxury', 'sandal', 'woman\n",
      "['searching', 'luxury', 'hermes', 'shoe', 'design'] ‚Üí ['design', 'hermes', 'luxury', 'searching', 'shoe\n",
      "['luxury', 'designer', 'sandal', 'online']         ‚Üí ['designer', 'luxury', 'online', 'sandal']\n",
      "\n",
      "====================================================================================================\n",
      "FINAL EXTRACTED KEYWORDS (Using Lemmatization Only):\n",
      "====================================================================================================\n",
      "\n",
      "1. Original: 'Hermes sandals for women designer luxury'\n",
      "   Keywords: ['designer', 'hermes', 'luxury', 'sandal', 'woman']\n",
      "\n",
      "2. Original: 'searching for luxury hermes shoe designs'\n",
      "   Keywords: ['design', 'hermes', 'luxury', 'searching', 'shoe']\n",
      "\n",
      "3. Original: 'visited luxury designer sandals online'\n",
      "   Keywords: ['designer', 'luxury', 'online', 'sandal']\n",
      "\n",
      "4. Original: 'Real estate property in Bangalore HSR'\n",
      "   Keywords: ['bangalore', 'estate', 'hsr', 'property', 'real']\n",
      "\n",
      "5. Original: 'Properties for sale in HSR layout locations'\n",
      "   Keywords: ['hsr', 'layout', 'location', 'property', 'sale']\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "KEYWORD SIMILARITY ANALYSIS:\n",
      "====================================================================================================\n",
      "\n",
      "Text 1: 'Hermes sandals for women designer luxury'\n",
      "  Keywords: ['designer', 'hermes', 'luxury', 'sandal', 'woman']\n",
      "\n",
      "Text 2: 'searching for luxury hermes shoe designs'\n",
      "  Keywords: ['design', 'hermes', 'luxury', 'searching', 'shoe']\n",
      "  Similarity with Text 1: 25.00%\n",
      "\n",
      "Text 3: 'visited luxury designer sandals online'\n",
      "  Keywords: ['designer', 'luxury', 'online', 'sandal']\n",
      "  Similarity with Text 1: 50.00%\n",
      "  Similarity with Text 2: 12.50%\n",
      "\n",
      "Interpretation:\n",
      "  ‚Ä¢ Texts with similar keywords (>30%) will be clustered together\n",
      "  ‚Ä¢ Related searches about 'hermes' and 'sandals' will form clusters\n",
      "  ‚Ä¢ Real estate searches form separate clusters\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate NLTK preprocessing techniques\n",
    "print(\"=\" * 100)\n",
    "print(\"NLTK TEXT PREPROCESSING DEMONSTRATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "test_texts = [\n",
    "    \"Hermes sandals for women designer luxury\",\n",
    "    \"searching for luxury hermes shoe designs\",\n",
    "    \"visited luxury designer sandals online\",\n",
    "    \"Real estate property in Bangalore HSR\",\n",
    "    \"Properties for sale in HSR layout locations\",\n",
    "]\n",
    "\n",
    "print(\"\\nText Processing Pipeline:\\n\")\n",
    "print(f\"{'Original Text':<50} ‚Üí {'Tokens':<50}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for text in test_texts[:3]:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    print(f\"{text:<50} ‚Üí {tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(f\"{'After Tokenization + Stopword Removal':<50} ‚Üí {'After Lemmatization':<50}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for text in test_texts[:3]:\n",
    "    # Step-by-step processing\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [t for t in tokens if t.isalnum() and t not in stop_words and len(t) > 2]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    \n",
    "    print(f\"{str(filtered_tokens):<50} ‚Üí {str(lemmatized)[:49]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 100)\n",
    "print(f\"{'Lemmatized':<50} ‚Üí {'Final Keywords':<50}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for text in test_texts[:3]:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [t for t in tokens if t.isalnum() and t not in stop_words and len(t) > 2]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    keywords = set(lemmatized)\n",
    "    \n",
    "    print(f\"{str(lemmatized):<50} ‚Üí {str(sorted(keywords))[:49]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL EXTRACTED KEYWORDS (Using Lemmatization Only):\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    keywords = extract_keywords_nltk(text)\n",
    "    print(f\"\\n{i}. Original: '{text}'\")\n",
    "    print(f\"   Keywords: {sorted(keywords)}\")\n",
    "\n",
    "# Demonstrate similarity calculation\n",
    "print(\"\\n\\n\" + \"=\" * 100)\n",
    "print(\"KEYWORD SIMILARITY ANALYSIS:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "text1_keywords = extract_keywords_nltk(test_texts[0])\n",
    "text2_keywords = extract_keywords_nltk(test_texts[1])\n",
    "text3_keywords = extract_keywords_nltk(test_texts[2])\n",
    "\n",
    "similarity_1_2 = calculate_keyword_similarity(text1_keywords, text2_keywords)\n",
    "similarity_1_3 = calculate_keyword_similarity(text1_keywords, text3_keywords)\n",
    "similarity_2_3 = calculate_keyword_similarity(text2_keywords, text3_keywords)\n",
    "\n",
    "print(f\"\\nText 1: '{test_texts[0]}'\")\n",
    "print(f\"  Keywords: {sorted(text1_keywords)}\\n\")\n",
    "\n",
    "print(f\"Text 2: '{test_texts[1]}'\")\n",
    "print(f\"  Keywords: {sorted(text2_keywords)}\")\n",
    "print(f\"  Similarity with Text 1: {similarity_1_2:.2%}\\n\")\n",
    "\n",
    "print(f\"Text 3: '{test_texts[2]}'\")\n",
    "print(f\"  Keywords: {sorted(text3_keywords)}\")\n",
    "print(f\"  Similarity with Text 1: {similarity_1_3:.2%}\")\n",
    "print(f\"  Similarity with Text 2: {similarity_2_3:.2%}\\n\")\n",
    "\n",
    "print(f\"Interpretation:\")\n",
    "print(f\"  ‚Ä¢ Texts with similar keywords (>30%) will be clustered together\")\n",
    "print(f\"  ‚Ä¢ Related searches about 'hermes' and 'sandals' will form clusters\")\n",
    "print(f\"  ‚Ä¢ Real estate searches form separate clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54c94185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "EXAMPLE SEARCH CLUSTERS - DETAILED VIEW (WITH NLTK ANALYSIS)\n",
      "====================================================================================================\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "CLUSTER 1 (Size: 1 entries)\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üè∑Ô∏è  NLTK-Extracted Keywords:\n",
      "     Lemmatized:     banking, event, investment, london, networking\n",
      "\n",
      "‚è±Ô∏è  Time Span: 0.0 minutes\n",
      "üìÖ Activities: {'search_query': 1}\n",
      "\n",
      "üìù Entries in cluster:\n",
      "\n",
      "   1. [2017-06-08 16:45:50] SEARCH_QUERY\n",
      "      Title: Searched for investment banking networking events london\n",
      "      Query: investment banking networking events london\n",
      "      Keywords: banking, event, investment, london, networking\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "CLUSTER 2 (Size: 1 entries)\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üè∑Ô∏è  NLTK-Extracted Keywords:\n",
      "     Lemmatized:     blackstone, event, networking, woman\n",
      "\n",
      "‚è±Ô∏è  Time Span: 0.0 minutes\n",
      "üìÖ Activities: {'search_query': 1}\n",
      "\n",
      "üìù Entries in cluster:\n",
      "\n",
      "   1. [2017-06-08 16:48:12] SEARCH_QUERY\n",
      "      Title: Searched for blackstone's women networking event\n",
      "      Query: blackstone's women networking event\n",
      "      Keywords: blackstone, event, networking, woman\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "CLUSTER 3 (Size: 2 entries)\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üè∑Ô∏è  NLTK-Extracted Keywords:\n",
      "     Lemmatized:     blackstone, home\n",
      "\n",
      "‚è±Ô∏è  Time Span: 0.0 minutes\n",
      "üìÖ Activities: {'search_query': 1, 'page_visit': 1}\n",
      "\n",
      "üìù Entries in cluster:\n",
      "\n",
      "   1. [2017-06-08 16:51:08] SEARCH_QUERY\n",
      "      Title: Searched for blackstone\n",
      "      Query: blackstone\n",
      "      Keywords: blackstone\n",
      "\n",
      "   2. [2017-06-08 16:51:10] PAGE_VISIT\n",
      "      Title: Visited Blackstone: Home\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "CLUSTER 4 (Size: 1 entries)\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üè∑Ô∏è  NLTK-Extracted Keywords:\n",
      "     Lemmatized:     face\n",
      "\n",
      "‚è±Ô∏è  Time Span: 0.0 minutes\n",
      "üìÖ Activities: {'search_query': 1}\n",
      "\n",
      "üìù Entries in cluster:\n",
      "\n",
      "   1. [2017-06-08 17:04:43] SEARCH_QUERY\n",
      "      Title: Searched for face\n",
      "      Query: face\n",
      "      Keywords: face\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "CLUSTER 5 (Size: 1 entries)\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üè∑Ô∏è  NLTK-Extracted Keywords:\n",
      "     Lemmatized:     facebook, log, sign\n",
      "\n",
      "‚è±Ô∏è  Time Span: 0.0 minutes\n",
      "üìÖ Activities: {'page_visit': 1}\n",
      "\n",
      "üìù Entries in cluster:\n",
      "\n",
      "   1. [2017-06-08 17:04:53] PAGE_VISIT\n",
      "      Title: Visited Facebook - log in or sign up\n"
     ]
    }
   ],
   "source": [
    "# Display example clusters with detailed information and NLTK-extracted keywords\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"EXAMPLE SEARCH CLUSTERS - DETAILED VIEW (WITH NLTK ANALYSIS)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if len(clusters_df) > 0:\n",
    "    unique_clusters = clusters_df['cluster_id'].unique()\n",
    "    num_to_show = min(5, len(unique_clusters))\n",
    "    \n",
    "    for cluster_num, cluster_id in enumerate(unique_clusters[:num_to_show], 1):\n",
    "        cluster_entries = clusters_df[clusters_df['cluster_id'] == cluster_id].copy()\n",
    "        \n",
    "        print(f\"\\n{'‚ñà' * 100}\")\n",
    "        print(f\"CLUSTER {int(cluster_id) + 1} (Size: {len(cluster_entries)} entries)\")\n",
    "        print(f\"{'‚ñà' * 100}\")\n",
    "        \n",
    "        # Extract NLTK keywords for the cluster\n",
    "        lemma_keywords = set()\n",
    "        \n",
    "        for _, row in cluster_entries.iterrows():\n",
    "            text = str(row['search_query']) if pd.notna(row['search_query']) else row['title']\n",
    "            lemma_kw = extract_keywords_nltk(text)\n",
    "            lemma_keywords.update(lemma_kw)\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è  NLTK-Extracted Keywords:\")\n",
    "        print(f\"     Lemmatized:     {', '.join(sorted(lemma_keywords)[:15])}\")\n",
    "        \n",
    "        # Time span\n",
    "        time_span = (cluster_entries['timestamp'].max() - cluster_entries['timestamp'].min()).total_seconds() / 60\n",
    "        print(f\"\\n‚è±Ô∏è  Time Span: {time_span:.1f} minutes\")\n",
    "        print(f\"üìÖ Activities: {cluster_entries['activity_type'].value_counts().to_dict()}\")\n",
    "        \n",
    "        print(f\"\\nüìù Entries in cluster:\")\n",
    "        for idx, (_, row) in enumerate(cluster_entries.iterrows(), 1):\n",
    "            timestamp_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            activity = row['activity_type']\n",
    "            title = row['title'][:70] if len(row['title']) > 70 else row['title']\n",
    "            \n",
    "            print(f\"\\n   {idx}. [{timestamp_str}] {activity.upper()}\")\n",
    "            print(f\"      Title: {title}\")\n",
    "            \n",
    "            if pd.notna(row['search_query']):\n",
    "                keywords = extract_keywords_nltk(row['search_query'])\n",
    "                print(f\"      Query: {row['search_query']}\")\n",
    "                print(f\"      Keywords: {', '.join(sorted(keywords))}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No clusters found with current parameters.\")\n",
    "    print(\"Try adjusting: keyword_similarity_threshold (lower), time_window_minutes (higher), min_cluster_size (lower)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74329c",
   "metadata": {},
   "source": [
    "## Section 3: Advanced NER-Based Entity Extraction Using OpenAI\n",
    "\n",
    "**Objective:** Use OpenAI's GPT-4o-mini model to perform sophisticated Named Entity Recognition and extract structured information (category, topic, entities) from search queries.\n",
    "\n",
    "**What this section does:**\n",
    "- Initializes OpenAI client with API credentials\n",
    "- Defines extraction prompt for structured information extraction\n",
    "- Implements extraction function that returns category, topic, and entities\n",
    "- Tests extraction on sample queries\n",
    "- Sets up caching system to avoid re-processing identical texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c768bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully!\n",
      "Using model: gpt-5-nano via responses API for NER extraction\n"
     ]
    }
   ],
   "source": [
    "# Install and configure OpenAI\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found. Set it in your environment or .env file.\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"OpenAI client initialized successfully!\")\n",
    "print(\"Using model: gpt-5-nano via responses API for NER extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7a3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING OPENAI NER EXTRACTION ===\n",
      "\n",
      "Query: Hermes sandals for women\n",
      "  Category: Fashion & Accessories\n",
      "  Topic: Sandals\n",
      "  Items: ['Hermes']\n",
      "\n",
      "Query: Hermes sandals for women\n",
      "  Category: Fashion & Accessories\n",
      "  Topic: Sandals\n",
      "  Items: ['Hermes']\n",
      "\n",
      "Query: How to bake sourdough bread\n",
      "  Category: Books & Learning\n",
      "  Topic: Baking\n",
      "  Items: ['sourdough']\n",
      "\n",
      "Query: How to bake sourdough bread\n",
      "  Category: Books & Learning\n",
      "  Topic: Baking\n",
      "  Items: ['sourdough']\n",
      "\n",
      "Query: Real estate property in HSR Layout Bangalore\n",
      "  Category: Real Estate & Property\n",
      "  Topic: Property search\n",
      "  Items: ['HSR Layout', 'Bangalore']\n",
      "\n",
      "Query: Real estate property in HSR Layout Bangalore\n",
      "  Category: Real Estate & Property\n",
      "  Topic: Property search\n",
      "  Items: ['HSR Layout', 'Bangalore']\n",
      "\n",
      "Query: Children smartphone addiction research\n",
      "  Category: Wellness & Health\n",
      "  Topic: Smartphone addiction in children\n",
      "  Items: ['children', 'smartphones']\n",
      "\n",
      "Query: Children smartphone addiction research\n",
      "  Category: Wellness & Health\n",
      "  Topic: Smartphone addiction in children\n",
      "  Items: ['children', 'smartphones']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the extraction prompt for OpenAI\n",
    "EXTRACTION_SYSTEM_PROMPT = \"\"\"You are an expert at analyzing search queries and web page titles to extract structured information.\n",
    "\n",
    "For each input text, extract:\n",
    "1. CATEGORY: The main semantic category (choose from: Fashion & Accessories, Real Estate & Property, Technology & Innovation, Wellness & Health, Travel & Transportation, Business & Finance, News & Media, Shopping & Retail, Entertainment, Books & Learning, General Interest)\n",
    "2. TOPIC: The main subject or focus of the query/title\n",
    "3. ITEMS: Specific entities mentioned (brands, products, people, places, organizations)\n",
    "\n",
    "Examples:\n",
    "- \"Hermes sandals for women\" ‚Üí Category: Fashion & Accessories, Topic: Sandals, Items: [\"Hermes\"]\n",
    "- \"Real estate property in HSR Layout Bangalore\" ‚Üí Category: Real Estate & Property, Topic: Property search, Items: [\"HSR Layout\", \"Bangalore\"]\n",
    "- \"How to bake sourdough bread\" ‚Üí Category: Books & Learning, Topic: Baking, Items: [\"sourdough\"]\n",
    "- \"children using smartphones too often\" ‚Üí Category: Wellness & Health, Topic: Smartphone usage, Items: [\"children\", \"smartphones\"]\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{\n",
    "  \"category\": \"Category Name\",\n",
    "  \"topic\": \"Main topic\",\n",
    "  \"items\": [\"item1\", \"item2\"]\n",
    "}\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_entities_and_context(text):\n",
    "    \"\"\"\n",
    "    Extract named entities and categorize them using OpenAI gpt-5-nano via the Responses API.\n",
    "\n",
    "    Returns:\n",
    "    {\n",
    "        'category': str,\n",
    "        'topic': str,\n",
    "        'items': list,\n",
    "        'raw_text': str\n",
    "    }\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return {\n",
    "            'category': 'Uncategorized',\n",
    "            'topic': 'Unknown',\n",
    "            'items': [],\n",
    "            'raw_text': text\n",
    "        }\n",
    "    \n",
    "    def try_parse_json(json_str):\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            # Try to repair by truncating at the first unmatched closing brace\n",
    "            last_brace = json_str.rfind('}')\n",
    "            if last_brace != -1:\n",
    "                try:\n",
    "                    return json.loads(json_str[:last_brace+1])\n",
    "                except Exception as e2:\n",
    "                    pass\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        # Call OpenAI Responses API with gpt-5-nano (response_format not supported in this SDK build)\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-nano\",\n",
    "            instructions=EXTRACTION_SYSTEM_PROMPT + \"\\nReturn JSON with keys: category, topic, items.\",\n",
    "            input=f\"Extract structured information from: {text}\",\n",
    "            reasoning={\"effort\": \"low\"},\n",
    "        )\n",
    "        \n",
    "        # Use regex to extract the first JSON object from the response text\n",
    "        response_text = response.output_text\n",
    "        json_match = re.search(r'\\{[\\s\\S]*?\\}', response_text)\n",
    "        result = None\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            result = try_parse_json(json_str)\n",
    "            if result is None:\n",
    "                print(f\"JSON decode error (regex-extracted, repaired): Could not parse after repair.\")\n",
    "        else:\n",
    "            # Fallback: try to parse the whole response as JSON (legacy)\n",
    "            result = try_parse_json(response_text)\n",
    "            if result is None:\n",
    "                print(f\"JSON decode error (full text, repaired): Could not parse after repair.\")\n",
    "        \n",
    "        if result is None:\n",
    "            return {\n",
    "                'category': 'General Interest',\n",
    "                'topic': text[:30] if len(text) > 30 else text,\n",
    "                'items': [],\n",
    "                'raw_text': text\n",
    "            }\n",
    "        \n",
    "        # If result is a list, take the first dict\n",
    "        if isinstance(result, list):\n",
    "            if len(result) > 0 and isinstance(result[0], dict):\n",
    "                result = result[0]\n",
    "            else:\n",
    "                return {\n",
    "                    'category': 'General Interest',\n",
    "                    'topic': text[:30] if len(text) > 30 else text,\n",
    "                    'items': [],\n",
    "                    'raw_text': text\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'category': result.get('category', 'General Interest'),\n",
    "            'topic': result.get('topic', 'Not specified'),\n",
    "            'items': result.get('items', []),\n",
    "            'raw_text': text\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Fallback to simple keyword matching if API fails\n",
    "        print(f\"API error for '{text[:50]}...': {e}\")\n",
    "        return {\n",
    "            'category': 'General Interest',\n",
    "            'topic': text[:30] if len(text) > 30 else text,\n",
    "            'items': [],\n",
    "            'raw_text': text\n",
    "        }\n",
    "\n",
    "# Test the extraction function\n",
    "print(\"=== TESTING OPENAI NER EXTRACTION ===\\n\")\n",
    "test_queries = [\n",
    "    \"Hermes sandals for women\",\n",
    "    \"How to bake sourdough bread\",\n",
    "    \"Real estate property in HSR Layout Bangalore\",\n",
    "    \"Children smartphone addiction research\"\n",
    " ]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = extract_entities_and_context(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"  Category: {result['category']}\")\n",
    "    print(f\"  Topic: {result['topic']}\")\n",
    "    print(f\"  Items: {result['items']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0dbbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized cluster topic/category/entity extraction with OpenAI NER and page scraping, with progress display\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helper: check if URL is a PDF\n",
    "def is_pdf_url(url):\n",
    "    return url.lower().endswith('.pdf') or 'application/pdf' in url.lower()\n",
    "\n",
    "# Helper: fetch and return main text content from a URL (skip PDFs)\n",
    "def fetch_page_content(url, timeout=10):\n",
    "    try:\n",
    "        if is_pdf_url(url):\n",
    "            return None, 'PDF skipped'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "        ct = resp.headers.get('Content-Type', '')\n",
    "        if 'pdf' in ct or is_pdf_url(url):\n",
    "            return None, 'PDF skipped'\n",
    "        if 'text/html' not in ct:\n",
    "            return None, f'Non-HTML content: {ct}'\n",
    "        # crude main text extraction\n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        # Remove scripts/styles\n",
    "        for tag in soup(['script', 'style', 'noscript']):\n",
    "            tag.decompose()\n",
    "        text = ' '.join(soup.stripped_strings)\n",
    "        return text[:5000], None  # limit to 5k chars\n",
    "    except Exception as e:\n",
    "        return None, f'Error: {e}'\n",
    "\n",
    "# Helper: similarity between query and url/title\n",
    "def simple_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# Main cluster summarization function\n",
    "def summarize_cluster(cluster_df, openai_extractor, max_api_retries=3, api_delay=1.0):\n",
    "    queries = cluster_df[cluster_df['activity_type'] == 'search_query']['search_query'].dropna().tolist()\n",
    "    page_visits = cluster_df[cluster_df['activity_type'] == 'page_visit']['titleUrl'].dropna().tolist()\n",
    "    queries_text = ' '.join(queries)\n",
    "\n",
    "    # Collect content/status for every visited page in the cluster\n",
    "    page_entries = []\n",
    "    for url in page_visits:\n",
    "        if not isinstance(url, str):\n",
    "            continue\n",
    "        content, status = fetch_page_content(url)\n",
    "        page_entries.append({'url': url, 'content': content, 'status': status})\n",
    "\n",
    "    # Compose context that includes all queries and all visited pages\n",
    "    context_parts = []\n",
    "    if queries:\n",
    "        context_parts.append('Queries: ' + ' | '.join(queries))\n",
    "    for entry in page_entries:\n",
    "        url = entry['url']\n",
    "        content = entry['content']\n",
    "        status = entry['status']\n",
    "        if content:\n",
    "            context_parts.append(f'Website: {url}\\nContent: {content}')\n",
    "        else:\n",
    "            context_parts.append(f'Website: {url}\\nContent: {status or \"(unavailable)\"}')\n",
    "    context = '\\n'.join(context_parts)\n",
    "\n",
    "    # OpenAI NER extraction with retry\n",
    "    for attempt in range(max_api_retries):\n",
    "        try:\n",
    "            result = openai_extractor(context)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt < max_api_retries - 1:\n",
    "                time.sleep(api_delay * (2 ** attempt))\n",
    "            else:\n",
    "                result = {'category': 'Unknown', 'topic': 'Unknown', 'items': [], 'raw_text': context, 'error': str(e)}\n",
    "    return {\n",
    "        'category': result.get('category', 'Unknown'),\n",
    "        'topic': result.get('topic', 'Unknown'),\n",
    "        'items': result.get('items', []),\n",
    "        'raw_text': context\n",
    "    }\n",
    "\n",
    "# Wrapper for all clusters with progress bar\n",
    "def summarize_all_clusters(clusters_df, openai_extractor, max_workers=3):\n",
    "    cluster_results = {}\n",
    "    if 'cluster_id' not in clusters_df.columns:\n",
    "        print('No clusters to summarize.')\n",
    "        return cluster_results\n",
    "    grouped = list(clusters_df.groupby('cluster_id'))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for cid, group in grouped:\n",
    "            futures.append((cid, executor.submit(summarize_cluster, group, openai_extractor)))\n",
    "        for cid, future in tqdm(futures, desc='Extracting clusters', unit='cluster'):\n",
    "            try:\n",
    "                cluster_results[cid] = future.result()\n",
    "            except Exception as e:\n",
    "                cluster_results[cid] = {'category': 'Unknown', 'topic': 'Unknown', 'items': [], 'raw_text': '', 'error': str(e)}\n",
    "    return cluster_results\n",
    "\n",
    "# Usage example (requires OpenAI extractor function):\n",
    "# cluster_summaries = summarize_all_clusters(clusters_df, extract_entities_and_context, max_workers=3)\n",
    "# print(cluster_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ce2e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting clusters:   2%|‚ñè         | 574/28566 [04:24<7:58:28,  1.03s/cluster] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error (full text, repaired): Could not parse after repair.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting clusters:   7%|‚ñã         | 2131/28566 [15:51<2:11:30,  3.35cluster/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error (regex-extracted, repaired): Could not parse after repair.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting clusters:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 13138/28566 [1:52:12<1:44:38,  2.46cluster/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error (regex-extracted, repaired): Could not parse after repair.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting clusters:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16019/28566 [2:14:25<1:52:47,  1.85cluster/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error (regex-extracted, repaired): Could not parse after repair.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting clusters:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 18249/28566 [2:31:29<2:06:02,  1.36cluster/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error (regex-extracted, repaired): Could not parse after repair.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting clusters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28566/28566 [4:05:58<00:00,  1.94cluster/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster summaries saved to cluster_summaries.json (28566 clusters)\n"
     ]
    }
   ],
   "source": [
    "# Run cluster extraction and save results to JSON\n",
    "import json\n",
    "\n",
    "# Run extraction (this may take time and use OpenAI API credits)\n",
    "cluster_summaries = summarize_all_clusters(clusters_df, extract_entities_and_context, max_workers=6)\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = 'cluster_summaries.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(cluster_summaries, f, indent=2)\n",
    "\n",
    "print(f\"Cluster summaries saved to {output_path} ({len(cluster_summaries)} clusters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737de72",
   "metadata": {},
   "source": [
    "## Section 4: Summary and Generated Files\n",
    "\n",
    "**Objective:** Provide a summary of all output files generated by this analysis and usage instructions.\n",
    "\n",
    "**What this section does:**\n",
    "- Lists all generated files with descriptions and sizes\n",
    "- Shows when files were created and updated\n",
    "- Provides next steps and usage examples\n",
    "- Explains how to load and use the structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10f23edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATED FILES SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìÅ Output Files:\n",
      "\n",
      "‚úì cluster_summaries.json\n",
      "  Description: Complete analysis with categories, topics, entities for clusters\n",
      "  Size: 41.30 MB\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display summary of all generated files\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATED FILES SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "files_to_check = [\n",
    "    ('cluster_summaries.json', 'Complete analysis with categories, topics, entities for clusters')\n",
    "]\n",
    "\n",
    "print(\"\\nüìÅ Output Files:\\n\")\n",
    "\n",
    "for filename, description in files_to_check:\n",
    "    if os.path.exists(filename):\n",
    "        file_size = os.path.getsize(filename)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"‚úì {filename}\")\n",
    "        print(f\"  Description: {description}\")\n",
    "        print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"‚ö† {filename} - Not found\")\n",
    "        print(f\"  Description: {description}\")\n",
    "        print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
